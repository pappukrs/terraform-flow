name: Infrastructure Up

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'Type "yes" to confirm EKS cluster creation (takes 10-15 minutes)'
        required: true
        default: ''

env:
  AWS_REGION: ap-south-1

jobs:
  terraform-up:
    name: Create EKS Infrastructure
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirm == 'yes' }}
    timeout-minutes: 30
    defaults:
      run:
        working-directory: ./infra

    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        run: terraform init

      - name: Terraform Plan
        run: terraform plan -out=tfplan

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan

      - name: Get Outputs
        id: outputs
        run: |
          echo "cluster_name=$(terraform output -raw eks_cluster_name)" >> $GITHUB_OUTPUT
          echo "cluster_endpoint=$(terraform output -raw eks_cluster_endpoint)" >> $GITHUB_OUTPUT
          echo "ecr_url=$(terraform output -raw ecr_repository_url)" >> $GITHUB_OUTPUT
          echo "kubectl_config=$(terraform output -raw configure_kubectl)" >> $GITHUB_OUTPUT

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ steps.outputs.outputs.cluster_name }}

      - name: Install eksctl
        run: |
          # Install eksctl
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version

      - name: Debug Nodes
        run: |
          echo "::group::Get Nodes"
          kubectl get nodes -o wide
          echo "::endgroup::"
          
          echo "::group::Describe Nodes"
          kubectl describe nodes
          echo "::endgroup::"
          
          echo "Waiting for at least one node to be Ready..."
          # This loop waits until at least one node is in Ready state
          # We cannot use --all if there are 0 nodes, so we check count first
          for i in {1..30}; do
             NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
             if [ "$NODE_COUNT" -gt "0" ]; then
                echo "Nodes found: $NODE_COUNT"
                break
             fi
             echo "No nodes yet, waiting..."
             sleep 10
          done
          
          # Now wait for readiness
          kubectl wait --for=condition=Ready nodes --all --timeout=150s || echo "WARNING: Nodes not ready yet"

      - name: Install AWS Load Balancer Controller
        run: |
          # 1. Associate OIDC Provider (Safe to run multiple times)
          eksctl utils associate-iam-oidc-provider \
            --region ${{ env.AWS_REGION }} \
            --cluster ${{ steps.outputs.outputs.cluster_name }} \
            --approve

          # 2. Create IAM Policy (Idempotent check)
          # Update to main/v3 policy
          curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
          
          POLICY_ARN=$(aws iam list-policies --query 'Policies[?PolicyName==`AWSLoadBalancerControllerIAMPolicy`].Arn' --output text)
          
          if [ -z "$POLICY_ARN" ]; then
            echo "Creating IAM Policy..."
            POLICY_ARN=$(aws iam create-policy \
              --policy-name AWSLoadBalancerControllerIAMPolicy \
              --policy-document file://iam_policy.json \
              --query 'Policy.Arn' --output text)
          else
            echo "IAM Policy already exists: $POLICY_ARN"
            # Optional: Update the policy version if needed, but for now we assume it's okay or user deletes it
          fi
          
          # 3. Create Service Account (Clean Slate Strategy)
          # eksctl can get confused if K8s SA is missing but CF Stack exists.
          STACK_NAME="eksctl-${{ steps.outputs.outputs.cluster_name }}-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
          echo "Force cleaning CloudFormation stack: $STACK_NAME"
          
          # Disable termination protection first
          aws cloudformation update-termination-protection \
            --no-enable-termination-protection \
            --stack-name $STACK_NAME \
            --region ${{ env.AWS_REGION }} || echo "Stack doesn't exist or protection already disabled"
          
          aws cloudformation delete-stack --stack-name $STACK_NAME --region ${{ env.AWS_REGION }}
          
          echo "Waiting for stack deletion to complete..."
          aws cloudformation wait stack-delete-complete --stack-name $STACK_NAME --region ${{ env.AWS_REGION }} || echo "Stack did not exist or already deleted"

          echo "Creating NEW ServiceAccount..."
          eksctl create iamserviceaccount \
            --cluster=${{ steps.outputs.outputs.cluster_name }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --role-name AmazonEKSLoadBalancerControllerRole \
            --attach-policy-arn=$POLICY_ARN \
            --approve \
            --region=${{ env.AWS_REGION }} \
            --override-existing-serviceaccounts

          # Verify SA creation
          echo "Verifying ServiceAccount existence..."
          if ! kubectl get sa aws-load-balancer-controller -n kube-system; then
             echo "âŒ ServiceAccount was not created! eksctl failed silently?"
             exit 1
          fi
          echo "âœ… ServiceAccount verified."

          # 4. Cleanup & Install Helm Chart
          echo "Cleaning up old controller installation..."
          helm uninstall aws-load-balancer-controller -n kube-system || true
          kubectl delete deployment aws-load-balancer-controller -n kube-system --ignore-not-found
          kubectl delete mutatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found
          kubectl delete validatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found
          
          echo "Installing Helm Chart..."
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ steps.outputs.outputs.cluster_name }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller

      - name: Wait for Load Balancer Controller
        run: |
          echo "Waiting for AWS Load Balancer Controller to be ready..."
          if ! kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s; then
            echo "âŒ Controller failed to start! Debugging info:"
            
            echo "::group::Cluster Events (Last 50)"
            kubectl get events -A --sort-by='.lastTimestamp' | tail -n 50
            echo "::endgroup::"

            echo "::group::Describe ReplicaSet"
            kubectl describe replicaset -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            echo "::endgroup::"
            
            echo "::group::Describe Deployment"
            kubectl describe deployment aws-load-balancer-controller -n kube-system
            echo "::endgroup::"
            
            echo "::group::Get Pods"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            echo "::endgroup::"
            
            echo "::group::Describe Pods"
            kubectl describe pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            echo "::endgroup::"
            
            echo "::group::Pod Logs"
            kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50
            echo "::endgroup::"
            
            exit 1
          fi

      - name: Verify cluster
        run: |
          kubectl get nodes
          kubectl get pods -A

      - name: Summary
        run: |
          echo "## EKS Infrastructure Created! ðŸš€" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Cluster Name:** \`${{ steps.outputs.outputs.cluster_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Cluster Endpoint:** \`${{ steps.outputs.outputs.cluster_endpoint }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**ECR Repository:** \`${{ steps.outputs.outputs.ecr_url }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Configure kubectl locally:" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`bash" >> $GITHUB_STEP_SUMMARY
          echo "${{ steps.outputs.outputs.kubectl_config }}" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âš ï¸ **Next Step:** Push to main branch to deploy your application!" >> $GITHUB_STEP_SUMMARY
